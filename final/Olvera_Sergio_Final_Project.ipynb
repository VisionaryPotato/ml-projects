{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Olvera_Sergio_Final_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intoduction\n",
        "For my final project, I'm going to implement two different machine learining approaches to see how they differ from one another.\n",
        "From this, I hope to draw a conclusion on what approach works best for the dataset. \n",
        "\n",
        "## Dataset \n",
        "This project will primarly will be using the [scikit-learn diabetes dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset). I will consider this project successful if I could test different machine learning models and identify which model works best and know *why* it works best."
      ],
      "metadata": {
        "id": "FNLnJQRbZRpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Packages\n"
      ],
      "metadata": {
        "id": "wUX8c413Ujcj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EyXc6Xa7UWow"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "%matplotlib inline\n",
        "\n",
        "from numpy.random import default_rng\n",
        "from sklearn import datasets # Where our diabetes datasets lives.\n",
        "from sklearn.model_selection import train_test_split # used for splitting our data\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Data Set\n",
        "This dataset comes from sklearns diabetes dataset. Instead of using the dataset provided by sklearns, I got the [data](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt) from the [source](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html) directly as the data from sklearn was standardized."
      ],
      "metadata": {
        "id": "PhSzpwstUhQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_data = datasets.load_diabetes()\n",
        "data = pd.read_csv('final/diabetes_data.csv')\n",
        "data.columns = ['age',\t'sex',\t'bmi',\t'bp',\t's1',\t's2', 's3',\t's4',\t's5',\t's6',\t'dp']\n",
        "data.head()"
      ],
      "metadata": {
        "id": "-IP2m5MfkxGJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "2e7c848b-9e18-4c75-cdaf-615c49ddb61c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ec994ea25b91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdiabetes_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_diabetes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'final/diabetes_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m'sex'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m'bmi'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m'bp'\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;34m's1'\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;34m's2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's3'\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0;34m's4'\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;34m's5'\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;34m's6'\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;34m'dp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'final/diabetes_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could take a quick look at the shape of the data just so we could visualize how it's structured, and the correlation coefficients of of the differend feature variables"
      ],
      "metadata": {
        "id": "31KLbIJJpjwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "IdZYowVWpsb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Interpritation\n",
        "### We can see in the shape of the data that there are 442 instances(samples) of data, with 10 feature variables. \n",
        "\n",
        "### These feature variables are:\n",
        "*   **age:** (in years)\n",
        "*   **sex:**\n",
        "*   **bmi:**: Body mass index\n",
        "*   **bp:** average blood pressure\n",
        "*   **s1-tc:** total serum cholesterol\n",
        "*   **s2-ldl:** low-density lipoproteiens\n",
        "*   **s3-hdl:** high-density lipoproteins\n",
        "*   **s4-tch:** total cholesterol / HDL\n",
        "*   **s5-ltg:** possibly log of serum triglycerides level\n",
        "*   **s6-glu:** blood sugar level\n",
        "*   **dp:** Our target data represents the disease progression one year after baseline. This is our variable of interest.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2hTUjCy5Uh7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "The first two Machine Learning approaches that I will be focusing on this project consist of univariate and multivariate linear regression models. This data set consists of 10 feature variables, so it's important to see the predictions differ in both models. I will then compare the results with a K-Nearest-Neighbors model.\n",
        "\n",
        "*Note: I will be use Assignment 3 as a reference when building the models as it's a well done exemplar on univariate & mulitvariate regression algorithims.*\n",
        "\n"
      ],
      "metadata": {
        "id": "itYCk4D2tD0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.corr()"
      ],
      "metadata": {
        "id": "sIbg6zLvuFs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, Body Mass Index and s5 (possibly log of serum triglycerides levels) are highly correlated with the disease progression after a year with the correlation values of 0.586450 and 0.565883 respectively.\n",
        "\n",
        "For our univariate linear regression model, we will use **BMI** as our feature variable and **dp** as our comparison.\n",
        "\n",
        "Our first step is to write a function that fits our univariate linear regression model. *\n",
        "Note: I will use Assignemt 3 as a reference as the process is similar.*"
      ],
      "metadata": {
        "id": "PblvD6Qaxtju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def univariate_model(df, feature):\n",
        "  linear_regression = LinearRegression()\n",
        "  X = data[[feature]].values #Primarly for, but not limited to, BMI.\n",
        "  y = data['dp'].values;\n",
        "  linear_regression.fit(X,y)\n",
        "  return X, y, linear_regression\n"
      ],
      "metadata": {
        "id": "PIQB-PfAyKu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xp, yp, lrp = univariate_model(data, 'bmi')"
      ],
      "metadata": {
        "id": "IlYwPQL2NpQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have this univariate regression model, we could visuallize the regression model by using the generated linear regression model to predict."
      ],
      "metadata": {
        "id": "dyp7W1e7N3wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(X, y, lr, xlabel,title):\n",
        "    x_label = xlabel\n",
        "    y_label = 'Disease Progression'\n",
        "    y_pred = lr.predict(X)  # our predicted linear model with respect to X\n",
        "    fig = plt.figure(figsize=(12,5))\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.scatter(X, y, c=\"darkgreen\")\n",
        "    ax.plot(X,y_pred,c=\"darkred\") # 'o' reformats the plot to circles instead of lines.\n",
        "    ax.set_xlabel(x_label)\n",
        "    ax.set_ylabel(y_label)\n",
        "    ax.set_title(title)\n",
        "    return None\n",
        "\n",
        "def plot_regression_result_1(X, y, lr, xlabel):\n",
        "    y_true = data['dp'].values # Ground truth target values.\n",
        "    y_pred = lr.predict(X) # our predicted linear model with respect to X\n",
        "    title = (f'{xlabel} Model with Respect to Disease Progression')\n",
        "    plot_graph(X, y, lr, xlabel,title);\n",
        "    return None"
      ],
      "metadata": {
        "id": "7Fp02FhKOvUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "plot_regression_result_1(Xp, yp, lrp, 'bmi')"
      ],
      "metadata": {
        "id": "ZUt8rV-IPQ8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's important for us to analyze the regression loss of our predicted model. In essense, this will help us justify if linear regression is a viable approach to this dataset. \n",
        "\n",
        "I'm going to create a function that calculates the mean absolute error & root mean error so we could se"
      ],
      "metadata": {
        "id": "gyhqytwtQKAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def round_y(y):# Referenced Assignement 3\n",
        "# We rounded our regressions in order to get an integer to match with our data.\n",
        "# This is done so we could calculate a valid regression accuracy.\n",
        "  return np.round(y) \n",
        "\n",
        "def evaluate(X, y, lr):\n",
        "  y_true = data['dp'].values\n",
        "  y_pred = lr.predict(X)\n",
        "\n",
        "  accur = np.sum((round_y(y_pred) == y))/y.shape[0]\n",
        "\n",
        "  rmse = np.sqrt(mean_squared_error(y_true, y_pred)) # Mean root error regression loss.\n",
        "  mae = mean_absolute_error(y_true, y_pred) # Mean absolute error regression loss.\n",
        "  r2 = lr.score(X,y)\n",
        "  print(\"The root mean squared error is: %4.2f\" % rmse)\n",
        "  print(\"The mean absolute error is: %4.2f\" % mae)\n",
        "  print(f'Coefficient of determination(R2): {r2:5.2f}')\n",
        "  print(f\"Accuracy: {accur:5.3f}\")"
      ],
      "metadata": {
        "id": "CwwPCRRUPQ3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(Xp,yp,lrp)"
      ],
      "metadata": {
        "id": "sIKDku7UT3bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used such evaluation metrics to make judgement of this linear regression model to compare continuous values. In this case we compared BMI with disease progression.\n",
        "\n",
        "\n",
        "As we can see, our evaluation metrics for our regression models tells us that essentially our BMI regression model can predict the disease progression with an **average error of 51.80**. The mean squared error wasn't that helpful because it punished larger errors, therefore I calculated the root mean squared error as it's easier to understand. \n",
        "\n",
        "It's important to note that the accuracy on BMI is not good whatsoever. This is to be expected as our error metrics were on average very high; however, we could take a look at the correlation coefficient. We could see that the variance in BMI and disease progression in this linear regression model is only 0.344. \n",
        "\n",
        "\n",
        "*   R2 = 0: Meaning that the model always fails to predict target value\n",
        "*   R2 = 1: Meaning that the model perfectly predict target value\n",
        "\n",
        "We could actually see the different R2 values with the other feature variables."
      ],
      "metadata": {
        "id": "bufRgl8iZmgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_r2():\n",
        "  sum = 0;\n",
        "  r2_scores = []\n",
        "  for feature in data.columns.drop(['dp']):\n",
        "      Xa, ya, lra = univariate_model(data, feature)\n",
        "      y_pred = lra.predict(Xa) # predction model\n",
        "      r2 = lra.score(Xa,ya)\n",
        "      r2_scores.append([r2,feature]) # appends accuracy and feature to our accuracies list.\n",
        "\n",
        "  [print(f\"R2 score {r2[0]:5.2f} for feature {r2[1]}\") for r2 in r2_scores] # prints our score for each feat.\n",
        "\n",
        "\n",
        "calculate_r2()"
      ],
      "metadata": {
        "id": "Lu5WjWF1u12B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By looking at these predicted univariate regression r2 models scores, we can tell that the **bmi** and **s5** feature models have the strongest relationship with disease progression."
      ],
      "metadata": {
        "id": "wXU0epV42pOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generalize with Multivariate Linear Regression\n",
        "As we can see, our BMI by it self produces very low accuracies with high error rates. Now let's train our linear regression all of our feature variables.\n",
        "\n",
        "We're going to start of by creating a function that fits our multiple features.\n"
      ],
      "metadata": {
        "id": "D5D8A7kbDmd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multivariate_model(df):\n",
        "  X = df[df.columns.drop(['dp'])] # all features except disease progression.\n",
        "  y = df['dp'] # our target value\n",
        "  model = LinearRegression();\n",
        "  model.fit(X,y)\n",
        "  return X, y, model"
      ],
      "metadata": {
        "id": "QPftsshDDlxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xm, ym, lrm = multivariate_model(data)\n",
        "\n",
        "y_pred = np.round(lrm.predict(Xm))\n",
        "evaluate(Xm,ym,lrm)"
      ],
      "metadata": {
        "id": "AMCehXXAJlEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this dataset, we can determine that,by using sklearn's linear regression model, it's only able to successfuly make \"good\" predictions 52% of the time when working with multivariate data. "
      ],
      "metadata": {
        "id": "pFObQ4tzH1VM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#K-Nearest-Neighbors Regressor\n",
        "Let's implement a K-nearest neighbor algorithm to see if there's a better change in our coefficient determination. This algorithm also allows us to generalize the data, so we'll be able to include all of the 10 features instead of just looking at BMI."
      ],
      "metadata": {
        "id": "aZOwsUMFliR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup\n",
        "\n",
        "In order for our knn algorithm to work properly, we need to split the data into two subsets: training & testing.\n",
        "\n",
        "Due to the nature of the data, I'll have to write a function that does the splitting for us. This function will be utilizing the sklearn.\n",
        "\n",
        "When splitting this dataset:\n",
        "\n",
        "*   We're splitting data 50/50.\n",
        "*   Random State = 42.\n",
        "\n"
      ],
      "metadata": {
        "id": "g1p2TfTgqI8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split(df,size):\n",
        "  X = df[df.columns.drop(['dp'])].values # All features except for quality\n",
        "  y = df['dp'].values\n",
        "  return train_test_split(X,y,test_size=size,random_state=42)"
      ],
      "metadata": {
        "id": "kR9AS3mH0eWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we could implement the k-nn algorithm and see how the R2 metrics differ in both machine learning models."
      ],
      "metadata": {
        "id": "fwk2z7oT1309"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_nearest_neighbor (df, k) : # Reference from Assignment 3.\n",
        "  X_train, X_test, y_train, y_test = split(df,.5)\n",
        "  kn = KNeighborsRegressor(n_neighbors=k)\n",
        "  kn.fit(X_train, y_train) #fit training data\n",
        "  kn.predict(X_test)\n",
        "  r2 = kn.score(X_test, y_test)\n",
        "  print(f'R2 score with {k} neighbors: {r2}')\n",
        "  return r2"
      ],
      "metadata": {
        "id": "oOq2lNkKuSNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_neighbors = 10\n",
        "r2_scores = []\n",
        "for i in range(2,n_neighbors+1):\n",
        "  r2_scores.append(fit_nearest_neighbor(data,i))\n"
      ],
      "metadata": {
        "id": "TLLRCf8x2zL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average knn R2 score: {np.mean(r2_scores):5.3f}\")"
      ],
      "metadata": {
        "id": "u4Th5LABJ2kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Network Approach\n",
        "##Setup"
      ],
      "metadata": {
        "id": "FWF-XhLMH0Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.metrics import MeanAbsoluteError\n",
        "\n",
        "!pip install tensorflow-addons # - May need to install this.\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.metrics import RSquare # add-on metric for rsquare"
      ],
      "metadata": {
        "id": "vBN2PJOub1n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we've previously seen in both our sklearn linear regression model & knn, our coefficient of determination is possibly our best indicator on how well our model is training. As a result, I wanted to collect the R2 metrics in our neural network implementation. "
      ],
      "metadata": {
        "id": "gGL5IbkkvMDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Custom Metric Function\n",
        "I found an amazing metric function that calculates R2 metrics when using keras.I will be using this metric to see if our neural network model produces better R2 results. \n",
        "\n",
        "*Source: https://jmlb.github.io/ml/2017/03/20/CoeffDetermination_CustomMetric4Keras/*\n",
        "\n",
        "**Update: I'm actually using a [tensorflow add-on](https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/RSquare) that calculates the R2 metrics **"
      ],
      "metadata": {
        "id": "WTeOqsX4wMBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = Sequential()\n",
        "input_size = len(data.columns)-1\n",
        "model_1.add(Dense(10, input_dim = input_size, kernel_initializer='normal', activation='relu')) # 10 feature variables\n",
        "model_1.add(Dense(5, activation='relu')) # hidden layer\n",
        "model_1.add(Dense(1, activation='relu')) # single output"
      ],
      "metadata": {
        "id": "7XaBZYJnb5Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our next step is to compile the network. We'll use the Adam optimizer with a the loss function of mean squared error. It would be beneficial to take the metrics of the mean squared error and mean absolute error as it would be useful to compare it against a regular regression model with no regularization."
      ],
      "metadata": {
        "id": "JFZYN3hWcCBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = keras.losses.MeanSquaredError()\n",
        "opt = keras.optimizers.Adam()\n",
        "mae = MeanAbsoluteError()\n",
        "r_sqr_metric = RSquare(dtype=tf.float32, y_shape=(1,))\n",
        "# compile network\n",
        "model_1.compile(loss=loss_fn, optimizer=opt, metrics=[r_sqr_metric, mae])"
      ],
      "metadata": {
        "id": "qxL8E5IHb8E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can split the data (60% training, 40% test) and finally we will train the neural network with our validation data."
      ],
      "metadata": {
        "id": "MhTr8gclchAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = split(data,.6)\n",
        "history = model_1.fit(X_train,y_train,batch_size=5,validation_split=0.6,epochs=1000, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "haZXspAycKi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(metric_name, title,ylim=1):\n",
        "  plt.title(title)\n",
        "  plt.ylim(-1,ylim)\n",
        "  plt.plot(history.history[metric_name],color='blue',label=metric_name)\n",
        "  plt.plot(history.history['val_' + metric_name],color='green',label='val_' + metric_name)\n",
        "  plt.legend()\n",
        "\n",
        "plot_metrics(\"r_square\", \"R Squared\");"
      ],
      "metadata": {
        "id": "pwoWQSvTcIv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "\n",
        "\n",
        "\n",
        "By using these two machine learning approaches with this dataset, I was able to see how good -or not so good- the machine learning models did when trying to predict the target value. For this dataset, It would be more appropiate to use a linear regression model instead of a KNN model. We saw that the R2 score on the multivariate linear regression model was higher than the average, of up to 10, k-nn. I was mostly focused on the coefficient of correlation as the accuracy for this dataset was very low with a high error rate, so I created an instance of a Neural Network model. I primarly wanted to see if creating and training a neural network to solve this linear-regression dataset could further improve our results. I was glad to see that the model does improve the over-all relationship between our trained models and our variable of interest. All though, I would still to a non-neural-network implementation of a linear regression model as neural model is a bit too complex (and I assume resource intensive) just for an extra ~10% better R2 score predictions. It was an amazing experience applying these models to make decisions about real-world problems such as diabetes.\n"
      ],
      "metadata": {
        "id": "T1G3zv-YJylG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z9o2apOYJ9bF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}